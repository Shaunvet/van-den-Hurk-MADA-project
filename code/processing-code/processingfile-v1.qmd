---
title: "Processing CDC hypertension and CDV Mortality disease data from adults in the USA - Processing script"
author: "Shaun van den Hurk"
date: "2025-02-20"
output: html_document
---

This should be the first file to run when reproducing the full analysis.

This file is a continuation of the data processing and wrangling steps. 
The first document highlighted the steps from importing the Excel document, the intial exploration and how the data was cleaned and reduced in R to remove the redundant columns (columns with single values).
Some of the other columns were renamed to have the name of the column reflect what these single value columns had contained.
The dataframe was then saved as an RDS file.

The initial Excel sheet was removed from the repository because it was almost 300MB and this was not suitable for uploading to the GitHUB repository.
Therefore, the code in that first document will not run because the file was removed. But the file remains to show the steps that were executed. If desired, the source Excel document could be downloaded from the CDC and steps run.

This document will load the RDS file and continue with some of the data processing, namely providing a couple of figures and summary tables of the dataset.
We likely do not need to do much further cleaning as that was done in the other steps.


# Setup

Load needed packages. make sure they are installed.

```{r}
#install.packages("sf")

library(readxl) #for loading Excel files (in case we want to include an additional dataset)
library(dplyr) #for data processing/cleaning
library(tidyr) #for data processing/cleaning
library(skimr) #for nice visualization of data 
library(here) #to set paths
library(knitr) #for nice tables and data integration
library(corrplot) #for correlation matrix visualization
library(sf) #for geospatial analysis
library(ggpubr) #for advanced visualization tools
library(ggplot2) #for plots

```


# Data loading

Note that for functions that come from specific packages (instead of base R), I often specify both package and function like so:
package::function() that's not required one could just call the function specifying the package makes it clearer where the function "lives",
but it adds typing. You can do it either way.

```{r}
# path to data
# note the use of the here() package and not absolute paths
cleaned_hypertension_adult_mortality_location <- here::here("data","raw-data","cleaned_hypertens_adult_mortality.rds")
cleaned_hypertens_mortailty_adult <- readRDS(cleaned_hypertension_adult_mortality_location)
```


# Check data


We check the data again.

```{r}
dplyr::glimpse(cleaned_hypertens_mortailty_adult)
summary(cleaned_hypertens_mortailty_adult)

```



# Cleaning
From our summary we can see that there are still a lot of NA values in our dataset. 
We want to look at these more closely to see if there is any valuable information in these rows or if we should just remove them.

```{r}
#Use filter function to search for rows with NA values in the mortalities column.
NA_Data <- cleaned_hypertens_mortailty_adult |> filter(is.na(mortalities))
#Checking these NA values
summary(NA_Data)
print(NA_Data)
View(NA_Data)

```

There do not seem to be any data values attached to these NA values. So we will remove them and create a new dataset without them. 

```{r}
#Once again using the filter function to leave us the dataframe without NA values
hypertens_adult_filtered <- cleaned_hypertens_mortailty_adult |> filter(!is.na(mortalities))
summary(hypertens_adult_filtered)
skimr::skim(hypertens_adult_filtered)
print(hypertens_adult_filtered)
```

As an additional check we will look for any duplicates in the data:
```{r}
sum(duplicated(hypertens_adult_filtered)) # Count duplicates
```
We are happy that there are no duplicates in our dataset.

We see from the summary that time is a character but we want numeric values and not character. We will confirm and convert if it is not numeric. (Lubridate function could have been used if the dates included month and days and time instead of only years. But numeric is simplest in this case.)
```{r}
str(hypertens_adult_filtered$year) # Check type
#We see that time is a character (we saw this from the summary)
hypertens_adult_filtered <- hypertens_adult_filtered %>% 
  mutate(Year = as.numeric(Year))

```



We want a copy of this skimr data for a table for the mauscript. We will make edits of this later in the process.

```{r}
skim_summary_hypertens_filtered<-skimr::skim(hypertens_adult_filtered)|> as.data.frame()
kable(skim_summary_hypertens_filtered)
invisible(saveRDS(skim_summary_hypertens_filtered, file = here::here("results", "output", "processing", "skim_summary_hypertens_filtered.rds")))
print(skim_summary_hypertens_filtered)

```


Our filtered data is now down to 572572 observations (from the initial 1.1 million). We have also removed many of the unnecessary columns (down to 14 from the initial 24).

Our summaries and the skimr function help to provide a good overview for further evaluation of the data. 
We can see that our dataset contains a combination of numeric and character data. 
The numeric data includes the actual incidence rates of the hypertension-related cardiovascular disease death rates, as well as the confidence intervals that were used, and geographic coordinates.
Our character data includes the race, sex, age group, location data and the year.

This character data will be used as predictors of the mortality data as we break down these different variables and look for potential patterns and associations. The primary outcome of interest is the mortality rates but the important potential predictors are the location, year, race/ethnicity, and sex. AGe groups only provide two broad categories, namely 35 to 65 years old and 65 years and older, which is a bit limited but will allow for comparisons between these two groups.


Our skimr data shows that the mean of the hypertension cardiovascular-diseases mortalities is around 203, with a standard deviation of 223. 
We want to get an idea on the distribution of this data and will use a histogram for this.

```{r histogram mortalities}
#Generate a histogram of the mortality rates
mortalities_histogram <- hist(hypertens_adult_filtered$mortalities)
invisible(saveRDS(mortalities_histogram, file = here::here("results","output", "processing", "mortalities_histogram.rds")))
mortalities_histogram <- readRDS(here::here("results", "output","processing", "mortalities_histogram.rds"))
# Print the mortalities histogram
print(mortalities_histogram)
```


Our histogram depicts the hypertension-related cardiovasculat disease mortalities in the dataset, showing the number of mortalities and the frequency (count) of sprecific mortality values.
The mortality counts are clustered towards the lower end of the scale, with a strong right-skew (long tail to the right/higher values). 


However, I acknowledge an issue in the current structure of the data with different subsets of data together, which makes these summaries limited for understanding trends.

We will start by removing the total percent change values from the dataset and create a new dataset.
```{r}
hypertens_adult_filtered_no_percent <- hypertens_adult_filtered %>%
  filter(Data_Value_Type != "Total percent change")
```


We will stratify the data based upon: County-level overall trends, stratify by race and sex (together and separately). All of these are based on the dataset with the total percent change removed.
We will also create separate dataset with the percent change data from each county.
We will not stratify by age group because of how the data is organised, this will not summarise much information or will need further summaries.

Stratify by country-level overall:
```{r}
#Using the filter function to help us with stratification
county_overall <- hypertens_adult_filtered_no_percent %>%
  filter(sex == "Overall", race_ethnicity == "Overall") %>% # Keep only total county-level data
  select(LocationID,Year,LocationAbbr, LocationDesc,  mortalities, Data_Value_Unit, Data_Value_Type, Confidence_limit_Low, Confidence_limit_High, age_group, X_long, Y_lat)

```


Stratify by race and sex per county (removing totals and where both values are overall):
```{r}
county_race_sex <- hypertens_adult_filtered_no_percent %>%
  filter(!(sex == "Overall" & race_ethnicity == "Overall")) %>% # Remove rows where both sex and race are 'Overall'
select(LocationID, Year, LocationAbbr, LocationDesc, mortalities, Data_Value_Unit, Data_Value_Type, Confidence_limit_Low, Confidence_limit_High, age_group, race_ethnicity, sex, X_long, Y_lat)
```


To allow us to assess each of these alone we will make sex and race have their own stratifications too.

Stratify by race per county:
```{r}
county_race <- hypertens_adult_filtered_no_percent %>%
  filter(race_ethnicity != "Overall") %>% # Remove total values for race
  select(LocationID, Year, LocationAbbr, LocationDesc, mortalities, Data_Value_Unit, Data_Value_Type, Confidence_limit_Low, Confidence_limit_High, age_group, race_ethnicity, X_long, Y_lat)
```


Stratify by sex per county:
```{r}
county_sex <- hypertens_adult_filtered_no_percent %>%
  filter(sex != "Overall") %>% # Remove total values for sex
  select(LocationID, Year, LocationAbbr, LocationDesc, mortalities, Data_Value_Unit, Data_Value_Type, Confidence_limit_Low, Confidence_limit_High, age_group, sex, X_long, Y_lat)
```


We create a new dataset where we stratify by percent change per county. (this may also serve as reference since we are removing this from the other stratified datasets):
```{r}
percent_change_data <- hypertens_adult_filtered %>%
  filter(Data_Value_Type == "Total percent change") %>% # Keep only percent change data
  select(LocationID, Year, LocationAbbr, LocationDesc, mortalities, Data_Value_Unit, Confidence_limit_Low, Confidence_limit_High, age_group, race_ethnicity, sex, X_long, Y_lat)
```


We are happy that we have improved the cleaning steps of our data and will revisit some summaries after saving the data.


# Save data 
The cleaned data will be saved as a new RDS file which will go under the processed data. 
The different datasets will be saved individually and as one collective file.

```{r}
save_processed_data_location <- here::here("data", "processed-data")

saveRDS(hypertens_adult_filtered, file = file.path(save_processed_data_location, "processeddata.rds"))
```

```{r}
# Save individual datasets
saveRDS(hypertens_adult_filtered_no_percent, file = file.path(save_processed_data_location, "processeddata_no_percent.rds"))
saveRDS(county_overall, file = file.path(save_processed_data_location, "county_overall.rds"))
saveRDS(county_race_sex, file = file.path(save_processed_data_location, "county_race_sex.rds"))
saveRDS(county_race, file = file.path(save_processed_data_location, "county_race.rds"))
saveRDS(county_sex, file = file.path(save_processed_data_location, "county_sex.rds"))
saveRDS(percent_change_data, file = file.path(save_processed_data_location, "percent_change_data.rds"))
```

```{r}
# Save all datasets in a single RDS file
final_processed_data <- list(
  processed_data_no_percent = hypertens_adult_filtered_no_percent,
  county_overall = county_overall,
  county_race_sex = county_race_sex,
  county_race = county_race,
  county_sex = county_sex,
  percent_change_data = percent_change_data
)
saveRDS(final_processed_data, file = file.path(save_processed_data_location, "final_processed_data.rds"))
```



## Summary and Next Steps

- The data was cleaned and processed to result in a dataset and a few different variables that are much smaller and more manageable to work with.
- We checked for missing variables and checked overall trends and removed d data and variables that were not useful.
- We have saved the different created variables as a list saved into one variable.
- All processed data has been saved into a single RDS file (`final_processed_data.rds`) for use in exploratory data analysis.








